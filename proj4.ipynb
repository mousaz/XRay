{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef2bad8-a350-4b08-8300-e91d01ff0ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ANNU-10\\.cache\\kagglehub\\datasets\\raddar\\chest-xrays-indiana-university\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b7996e-e51c-43d6-9a50-353412da42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7e9aba-ddd7-456e-aa14-e16d20e3980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7466 entries, 0 to 7465\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   uid         7466 non-null   int64 \n",
      " 1   filename    7466 non-null   object\n",
      " 2   projection  7466 non-null   object\n",
      " 3   MeSH        7466 non-null   object\n",
      " 4   Problems    7466 non-null   object\n",
      " 5   image       7466 non-null   object\n",
      " 6   indication  7466 non-null   object\n",
      " 7   comparison  7466 non-null   object\n",
      " 8   findings    7466 non-null   object\n",
      " 9   impression  7466 non-null   object\n",
      " 10  report      7466 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 641.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "dataset_folder = path\n",
    "images_folder = dataset_folder + \"/images/images_normalized\"\n",
    "projections = pandas.read_csv(dataset_folder + \"/indiana_projections.csv\")\n",
    "reports = pandas.read_csv(dataset_folder + \"/indiana_reports.csv\")\n",
    "\n",
    "combined_dataset = projections.merge(reports, on=\"uid\", how=\"inner\")\n",
    "\n",
    "def IsNotAvailable(value):\n",
    "    return value.str.contains(\"unavailable\", case=False, na=False) \\\n",
    "        | value.str.contains(\"not available\", case=False, na=False) \\\n",
    "        | value.str.contains(\"none\", case=False, na=False)\n",
    "\n",
    "combined_dataset.loc[IsNotAvailable(combined_dataset[\"comparison\"]), \"comparison\"] = \"None\"\n",
    "\n",
    "combined_dataset[\"indication\"] = combined_dataset[\"indication\"].fillna(\"None\")\n",
    "combined_dataset[\"findings\"] = combined_dataset[\"findings\"].fillna(\"None\")\n",
    "combined_dataset[\"impression\"] = combined_dataset[\"impression\"].fillna(\"None\")\n",
    "combined_dataset[\"comparison\"] = combined_dataset[\"comparison\"].fillna(\"None\")\n",
    "combined_dataset[\"report\"] = (\n",
    "    \"Indication: \" + combined_dataset[\"indication\"].astype(str) + \"\\n\"\n",
    "    + \"Findings: \" + combined_dataset[\"findings\"].astype(str) + \"\\n\"\n",
    "    + \"Impression: \" + combined_dataset[\"impression\"].astype(str) + \"\\n\"\n",
    "    + \"Comparison: \" + combined_dataset[\"comparison\"].astype(str)\n",
    ")\n",
    "\n",
    "combined_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7f9581-a1e8-48dd-ae44-280b2279eb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>filename</th>\n",
       "      <th>projection</th>\n",
       "      <th>MeSH</th>\n",
       "      <th>Problems</th>\n",
       "      <th>image</th>\n",
       "      <th>indication</th>\n",
       "      <th>comparison</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>None</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>Indication: Positive TB test\\nFindings: The ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>None</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>Indication: Positive TB test\\nFindings: The ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n",
       "      <td>Cardiomegaly;Pulmonary Artery</td>\n",
       "      <td>Chest, 2 views, frontal and lateral</td>\n",
       "      <td>Preop bariatric surgery.</td>\n",
       "      <td>None</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>No acute pulmonary findings.</td>\n",
       "      <td>Indication: Preop bariatric surgery.\\nFindings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n",
       "      <td>Cardiomegaly;Pulmonary Artery</td>\n",
       "      <td>Chest, 2 views, frontal and lateral</td>\n",
       "      <td>Preop bariatric surgery.</td>\n",
       "      <td>None</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>No acute pulmonary findings.</td>\n",
       "      <td>Indication: Preop bariatric surgery.\\nFindings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>rib pain after a XXXX, XXXX XXXX steps this XX...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No displaced rib fractures, pneumothorax, or p...</td>\n",
       "      <td>Indication: rib pain after a XXXX, XXXX XXXX s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                filename projection  \\\n",
       "0    1  1_IM-0001-4001.dcm.png    Frontal   \n",
       "1    1  1_IM-0001-3001.dcm.png    Lateral   \n",
       "2    2  2_IM-0652-1001.dcm.png    Frontal   \n",
       "3    2  2_IM-0652-2001.dcm.png    Lateral   \n",
       "4    3  3_IM-1384-1001.dcm.png    Frontal   \n",
       "\n",
       "                                                MeSH  \\\n",
       "0                                             normal   \n",
       "1                                             normal   \n",
       "2  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n",
       "3  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n",
       "4                                             normal   \n",
       "\n",
       "                        Problems                                image  \\\n",
       "0                         normal            Xray Chest PA and Lateral   \n",
       "1                         normal            Xray Chest PA and Lateral   \n",
       "2  Cardiomegaly;Pulmonary Artery  Chest, 2 views, frontal and lateral   \n",
       "3  Cardiomegaly;Pulmonary Artery  Chest, 2 views, frontal and lateral   \n",
       "4                         normal            Xray Chest PA and Lateral   \n",
       "\n",
       "                                          indication comparison  \\\n",
       "0                                   Positive TB test       None   \n",
       "1                                   Positive TB test       None   \n",
       "2                           Preop bariatric surgery.       None   \n",
       "3                           Preop bariatric surgery.       None   \n",
       "4  rib pain after a XXXX, XXXX XXXX steps this XX...       None   \n",
       "\n",
       "                                            findings  \\\n",
       "0  The cardiac silhouette and mediastinum size ar...   \n",
       "1  The cardiac silhouette and mediastinum size ar...   \n",
       "2  Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "3  Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "4                                               None   \n",
       "\n",
       "                                          impression  \\\n",
       "0                               Normal chest x-XXXX.   \n",
       "1                               Normal chest x-XXXX.   \n",
       "2                       No acute pulmonary findings.   \n",
       "3                       No acute pulmonary findings.   \n",
       "4  No displaced rib fractures, pneumothorax, or p...   \n",
       "\n",
       "                                              report  \n",
       "0  Indication: Positive TB test\\nFindings: The ca...  \n",
       "1  Indication: Positive TB test\\nFindings: The ca...  \n",
       "2  Indication: Preop bariatric surgery.\\nFindings...  \n",
       "3  Indication: Preop bariatric surgery.\\nFindings...  \n",
       "4  Indication: rib pain after a XXXX, XXXX XXXX s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ef11ed-b39c-4966-9fc6-142d4f233878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indication: Positive TB test\n",
      "Findings: The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.\n",
      "Impression: Normal chest x-XXXX.\n",
      "Comparison: None\n",
      "-----\n",
      "Indication: Positive TB test\n",
      "Findings: The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.\n",
      "Impression: Normal chest x-XXXX.\n",
      "Comparison: None\n",
      "-----\n",
      "Indication: Preop bariatric surgery.\n",
      "Findings: Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX.\n",
      "Impression: No acute pulmonary findings.\n",
      "Comparison: None\n",
      "-----\n",
      "Indication: Preop bariatric surgery.\n",
      "Findings: Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX.\n",
      "Impression: No acute pulmonary findings.\n",
      "Comparison: None\n",
      "-----\n",
      "Indication: rib pain after a XXXX, XXXX XXXX steps this XXXX. Pain to R back, R elbow and R rib XXXX, no previous heart or lung hx, non-XXXX, no hx ca\n",
      "Findings: None\n",
      "Impression: No displaced rib fractures, pneumothorax, or pleural effusion identified. Well-expanded and clear lungs. Mediastinal contour within normal limits. No acute cardiopulmonary abnormality identified.\n",
      "Comparison: None\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for r in combined_dataset[\"report\"].head(5).to_list():\n",
    "    print(r)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee42af7-566e-4d47-a5b2-70896db8adae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>Indication: Positive TB test\\nFindings: The ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>Indication: Positive TB test\\nFindings: The ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Indication: Preop bariatric surgery.\\nFindings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Indication: Preop bariatric surgery.\\nFindings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>Indication: rib pain after a XXXX, XXXX XXXX s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>3997_IM-2048-1002.dcm.png</td>\n",
       "      <td>Indication: XXXX-year-old male with positive P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7462</th>\n",
       "      <td>3998_IM-2048-1001.dcm.png</td>\n",
       "      <td>Indication: tuberculosis positive PPD\\nFinding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>3998_IM-2048-1002.dcm.png</td>\n",
       "      <td>Indication: tuberculosis positive PPD\\nFinding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>3999_IM-2049-1001.dcm.png</td>\n",
       "      <td>Indication: This is a XXXX-year-old female pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>3999_IM-2049-2001.dcm.png</td>\n",
       "      <td>Indication: This is a XXXX-year-old female pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "0        1_IM-0001-4001.dcm.png   \n",
       "1        1_IM-0001-3001.dcm.png   \n",
       "2        2_IM-0652-1001.dcm.png   \n",
       "3        2_IM-0652-2001.dcm.png   \n",
       "4        3_IM-1384-1001.dcm.png   \n",
       "...                         ...   \n",
       "7461  3997_IM-2048-1002.dcm.png   \n",
       "7462  3998_IM-2048-1001.dcm.png   \n",
       "7463  3998_IM-2048-1002.dcm.png   \n",
       "7464  3999_IM-2049-1001.dcm.png   \n",
       "7465  3999_IM-2049-2001.dcm.png   \n",
       "\n",
       "                                                 report  \n",
       "0     Indication: Positive TB test\\nFindings: The ca...  \n",
       "1     Indication: Positive TB test\\nFindings: The ca...  \n",
       "2     Indication: Preop bariatric surgery.\\nFindings...  \n",
       "3     Indication: Preop bariatric surgery.\\nFindings...  \n",
       "4     Indication: rib pain after a XXXX, XXXX XXXX s...  \n",
       "...                                                 ...  \n",
       "7461  Indication: XXXX-year-old male with positive P...  \n",
       "7462  Indication: tuberculosis positive PPD\\nFinding...  \n",
       "7463  Indication: tuberculosis positive PPD\\nFinding...  \n",
       "7464  Indication: This is a XXXX-year-old female pat...  \n",
       "7465  Indication: This is a XXXX-year-old female pat...  \n",
       "\n",
       "[7466 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dataset = combined_dataset.loc[:, (\"filename\", \"report\")]\n",
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57bdc739-7e88-4d32-88e0-d635556ec82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5972, 11), Test shape: (1494, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(combined_dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e70f10d-2b7f-405d-98b6-514f9700bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageReportDataset(Dataset):\n",
    "    def __init__(self, dataset, img_dir, tokenizer, transform=None):\n",
    "        self.data = dataset\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.loc[idx, \"filename\"])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        report = self.data.loc[idx, \"report\"] + \"<|endoftext|>\"\n",
    "        # inputs = self.tokenizer(report, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        # return image, inputs[\"input_ids\"].squeeze(), inputs[\"attention_mask\"].squeeze()\n",
    "        inputs = tokenizer(report, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "        labels[:-1] = input_ids[1:]\n",
    "        return image, input_ids, labels\n",
    "        # return image, inputs[\"input_ids\"].squeeze(), inputs[\"attention_mask\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b48e0a-47d6-4141-ab0f-f75572fddafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torchvision import transforms\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image = [i[0] for i in batch]\n",
    "    input_ids = [i[1] for i in batch]\n",
    "    labels = [i[2] for i in batch]\n",
    "    image = torch.stack(image, dim=0)\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    \n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": labels},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    \n",
    "    mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    labels[mask==0] = -100\n",
    "    return image, input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a27051c-321a-4216-8e0f-13213a2a9271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5497,  3299,    25, 27713,    55, 27713,    55, 43172,  4048,    11,\n",
      "         1478,  2745, 10423,   351, 27713,    55,    11,   256, 35586,  9517,\n",
      "          544,    11,  1790,  1108,   286,   198, 16742,   654,    25,  6045,\n",
      "          198, 26950,  2234,    25,  1400,  4075,  4369,    13,   198, 50249,\n",
      "         1653,    25,  6045, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256])\n",
      "tensor([ 3299,    25, 27713,    55, 27713,    55, 43172,  4048,    11,  1478,\n",
      "         2745, 10423,   351, 27713,    55,    11,   256, 35586,  9517,   544,\n",
      "           11,  1790,  1108,   286,   198, 16742,   654,    25,  6045,   198,\n",
      "        26950,  2234,    25,  1400,  4075,  4369,    13,   198, 50249,  1653,\n",
      "           25,  6045, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "dset = ImageReportDataset(train_df, images_folder, tokenizer, transform)\n",
    "dloader = DataLoader(dset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "_, c, l = next(iter(dloader))\n",
    "print(c[0])\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e57cc9-2676-4a96-b913-d435abcac647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension should be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        # q,k,v shape individually: batch_size x seq_len x embed_dim\n",
    "        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "        q = q.view(b, t, self.n_heads,self.head_size).permute(0, 2, 1, 3) # batch x n_heads x seq_len x head_dim\n",
    "        k = k.view(b, t, self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, t, self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        qk_t = qk_t.masked_fill(self.mask[:, :, :t, :t] == 0, float(\"-inf\"))\n",
    "        qk_t = F.softmax(qk_t, dim=-1)\n",
    "        weights = self.attn_dropout(qk_t)\n",
    "\n",
    "        attention = weights @ v # batch x n_heads x t x head_size\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c) # batch x t x embed_dim\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ba63448-741a-4403-b211-3e00ec0f703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension by be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "\n",
    "        self.q = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b, t, c = q.shape\n",
    "\n",
    "        q = self.q(q)\n",
    "        k = self.k(k)\n",
    "        v = self.v(v)\n",
    "\n",
    "        q = q.view(b, q.size(1), self.n_heads,self.head_size).permute(0, 2, 1, 3) # batch x n_heads x seq_len x head_dim\n",
    "        k = k.view(b, k.size(1), self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, v.size(1), self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        qk_t = F.softmax(qk_t,dim=-1)\n",
    "        weights = self.attn_dropout(qk_t)\n",
    "\n",
    "        attention = weights @ v # batch x n_heads x t x head_size\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c) # batch x t x embed_dim\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f49162-e542-42ca-8e4d-a05231d23d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.mlp_ratio = config.mlp_ratio\n",
    "        self.mlp_dropout = config.mlp_dropout\n",
    "\n",
    "        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n",
    "        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(self.mlp_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf1011a-b533-4204-abf7-e5bc92fa1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ln_1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "        self.ln_3 = nn.LayerNorm(self.embed_dim)\n",
    "        self.cross_attn = GPT2CrossAttention(config)\n",
    "\n",
    "    def forward(self,x,enc_out):\n",
    "        x = x+self.attn(self.ln_1(x))\n",
    "        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out)\n",
    "        x = x+self.mlp(self.ln_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "173729f5-8977-472c-baf6-74adf0b3b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        vit = create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = vit.cls_token\n",
    "        embed_len = num_patches + vit.num_prefix_tokens\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = nn.Dropout(p=0.)\n",
    "\n",
    "        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n",
    "            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n",
    "            drop = nn.Dropout(config.emb_dropout),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n",
    "            ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def _pos_embed(self,x):\n",
    "        pos_embed = self.pos_embed\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + pos_embed\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def pretrained_layers_trainable(self,trainable=False):\n",
    "        layers = [\n",
    "            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n",
    "            self.transformer.wte, self.transformer.wpe,\n",
    "            self.transformer.ln_f, self.lm_head\n",
    "        ]\n",
    "        gpt_layers = [[\n",
    "            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n",
    "            self.transformer.h[i].attn,self.transformer.h[i].mlp\n",
    "        ] for i in range(self.config.depth)]\n",
    "        for l in gpt_layers:\n",
    "            layers.extend(l)\n",
    "\n",
    "        for layer in layers:\n",
    "            if not isinstance(layer,nn.Parameter):\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = trainable\n",
    "            else:\n",
    "                layer.requires_grad = trainable\n",
    "\n",
    "        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n",
    "        print(f'{total_frozen_params=}')\n",
    "\n",
    "    def unfreeze_gpt_layers(self,):\n",
    "        gpt_layers = [[\n",
    "            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n",
    "            self.transformer.h[i].attn,self.transformer.h[i].mlp\n",
    "        ] for i in range(self.config.depth)]\n",
    "        flatten = []\n",
    "        for l in gpt_layers:\n",
    "            flatten.extend(l)\n",
    "\n",
    "        for layer in flatten:\n",
    "            if not isinstance(layer,nn.Parameter):\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                layer.requires_grad = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(self, config):\n",
    "        model = VisionGPT2Model(config)\n",
    "        sd = model.state_dict()\n",
    "        keys = sd.keys()\n",
    "        ignore_matches = [\"blocks.\", \"cross_attn.\", \"ln_3\", \"cls_token\", \"pos_embed\", \"patch_embed.\", \".attn.mask\"]\n",
    "        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n",
    "        gpt_keys = [key for key in keys if key not in vit_keys]\n",
    "\n",
    "        gpt2_small = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        sd_hf = gpt2_small.state_dict()\n",
    "        hf_keys = sd_hf.keys()\n",
    "        hf_keys = [k for k in hf_keys if not k.endswith(\".attn.masked_bias\")]\n",
    "        hf_keys = [k for k in hf_keys if not k.endswith(\".attn.bias\")]\n",
    "        transposed = [\"attn.c_attn.weight\", \"attn.c_proj.weight\", \"mlp.c_fc.weight\", \"mlp.c_proj.weight\"]\n",
    "\n",
    "        for k in hf_keys:\n",
    "            if any(match in k for match in ignore_matches):\n",
    "                continue\n",
    "                \n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        model.load_state_dict(sd)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, image, input_ids, labels=None):\n",
    "\n",
    "        image = self.patch_embed(image)\n",
    "        image = self._pos_embed(image)\n",
    "\n",
    "        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n",
    "        pos_embs = torch.arange(0, input_ids.size(1)).to(input_ids.device)\n",
    "        positional_embeddings = self.transformer.wpe(pos_embs)\n",
    "        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n",
    "\n",
    "        for i in range(self.config.depth):\n",
    "            image = self.blocks[i](image)\n",
    "            input_ids = self.transformer.h[i](input_ids, image)\n",
    "\n",
    "        input_ids = self.transformer.ln_f(input_ids)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits = self.lm_head(input_ids)\n",
    "            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n",
    "            return loss\n",
    "\n",
    "        lm_logits = self.lm_head(input_ids[:, [-1], :])\n",
    "        return lm_logits\n",
    "\n",
    "    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            out = self(image,sequence)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            probs = F.softmax(out, dim=-1)\n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sequence = torch.cat([sequence,next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return sequence.cpu().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "facc39c8-cc4b-49ea-93b0-5b2a8471dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model_config, train_config, dls):\n",
    "        self.train_config = train_config\n",
    "        self.model_config = model_config\n",
    "        self.device = self.train_config.device\n",
    "\n",
    "        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        self.model.pretrained_layers_trainable(trainable=False)\n",
    "\n",
    "        print(f\"Trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}\")\n",
    "\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "        self.train_dl, self.val_dl = dls\n",
    "\n",
    "        total_steps = len(self.train_dl)\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n",
    "        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optim,\n",
    "            max_lr=self.train_config.lr,\n",
    "            epochs=self.train_config.epochs,\n",
    "            steps_per_epoch=total_steps\n",
    "        )\n",
    "\n",
    "#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "\n",
    "        self.metrics = pandas.DataFrame()\n",
    "        self.metrics[[\"train_loss\", \"train_perplexity\", \"val_loss\", \"val_perplexity\"]] = None\n",
    "\n",
    "        self.gen_tfms = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def save_model(self,):\n",
    "        self.train_config.model_path.mkdir(exist_ok=True)\n",
    "        sd = self.model.state_dict()\n",
    "        torch.save(self.model, self.train_config.model_path/\"captioner.pt\")\n",
    "\n",
    "    def load_best_model(self,):\n",
    "        sd = torch.load(self.train_config.model_path/\"captioner.pt\")\n",
    "        self.model.load_state_dict(sd)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        prog = tqdm(self.train_dl,total=len(self.train_dl))\n",
    "        running_loss = 0.\n",
    "\n",
    "        for image, input_ids, labels in prog:\n",
    "            with autocast():\n",
    "                image = image.to(self.device)\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                loss = self.model(image,input_ids,labels)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optim)\n",
    "                self.scaler.update()\n",
    "                self.sched.step()\n",
    "                self.optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                prog.set_description(f'train loss: {loss.item():.3f}')\n",
    "\n",
    "            del image, input_ids, labels, loss\n",
    "\n",
    "        train_loss = running_loss / len(self.train_dl)\n",
    "        train_pxp = np.exp(train_loss)\n",
    "\n",
    "        self.metrics.loc[epoch, [\"train_loss\", \"train_perplexity\"]] = (train_loss,train_pxp)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self, epoch):\n",
    "\n",
    "        prog = tqdm(self.val_dl,total=len(self.val_dl))\n",
    "\n",
    "        running_loss = 0.\n",
    "\n",
    "        for image, input_ids, labels in prog:\n",
    "\n",
    "            with autocast():\n",
    "                image = image.to(self.device)\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                loss = self.model(image,input_ids,labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                prog.set_description(f\"Valid loss: {loss.item():.3f}\")\n",
    "\n",
    "            del image, input_ids, labels, loss\n",
    "\n",
    "        val_loss = running_loss / len(self.val_dl)\n",
    "        val_pxp = np.exp(val_loss)\n",
    "\n",
    "        self.metrics.loc[epoch, [\"val_loss\", \"val_perplexity\"]] = (val_loss,val_pxp)\n",
    "\n",
    "        return val_pxp\n",
    "\n",
    "    def clean(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def fit(self,):\n",
    "        best_pxp = 1e9\n",
    "        best_epoch = -1\n",
    "        prog = tqdm(range(self.train_config.epochs))\n",
    "\n",
    "        for epoch in prog:\n",
    "            if epoch == self.train_config.freeze_epochs_gpt:\n",
    "                self.model.unfreeze_gpt_layers()\n",
    "                print(\"Unfreezing GPT2 entirely...\")\n",
    "\n",
    "            if epoch == self.train_config.freeze_epochs_all:\n",
    "                self.model.pretrained_layers_trainable(trainable=True)\n",
    "\n",
    "            self.model.train()\n",
    "            prog.set_description(\"Training\")\n",
    "            self.train_one_epoch(epoch)\n",
    "            self.clean()\n",
    "\n",
    "            self.model.eval()\n",
    "            prog.set_description(\"Validating\")\n",
    "            pxp = self.valid_one_epoch(epoch)\n",
    "            self.clean()\n",
    "\n",
    "            print(self.metrics.tail(1))\n",
    "\n",
    "            if pxp < best_pxp:\n",
    "                best_pxp = pxp\n",
    "                best_epoch = epoch\n",
    "                print(\"Saving best model...\")\n",
    "                self.save_model()\n",
    "\n",
    "        return {\n",
    "            \"best_perplexity\": best_pxp,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_caption(self, image, max_tokens=50, temperature=1.0, deterministic=False):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        image = self.gen_tfms(image=image)[\"image\"]\n",
    "        image = image.unsqueeze(0).to(self.device)\n",
    "        sequence = torch.ones(1, 1).to(device=self.device).long() * self.tokenizer.bos_token_id\n",
    "\n",
    "        caption = self.model.generate(\n",
    "            image,\n",
    "            sequence,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n",
    "\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d38da-e540-4469-b91b-2cefeb2392c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
