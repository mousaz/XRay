{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef2bad8-a350-4b08-8300-e91d01ff0ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\v-mziadeh\\.cache\\kagglehub\\datasets\\raddar\\chest-xrays-indiana-university\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b7996e-e51c-43d6-9a50-353412da42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7e9aba-ddd7-456e-aa14-e16d20e3980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7466 entries, 0 to 7465\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   uid         7466 non-null   int64 \n",
      " 1   filename    7466 non-null   object\n",
      " 2   projection  7466 non-null   object\n",
      " 3   MeSH        7466 non-null   object\n",
      " 4   Problems    7466 non-null   object\n",
      " 5   image       7466 non-null   object\n",
      " 6   indication  7466 non-null   object\n",
      " 7   comparison  7466 non-null   object\n",
      " 8   findings    7466 non-null   object\n",
      " 9   impression  7466 non-null   object\n",
      " 10  report      7466 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 641.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "dataset_folder = path\n",
    "images_folder = dataset_folder + \"/images/images_normalized\"\n",
    "projections = pandas.read_csv(dataset_folder + \"/indiana_projections.csv\")\n",
    "reports = pandas.read_csv(dataset_folder + \"/indiana_reports.csv\")\n",
    "\n",
    "combined_dataset = projections.merge(reports, on=\"uid\", how=\"inner\")\n",
    "\n",
    "def IsNotAvailable(value):\n",
    "    return value.str.contains(\"unavailable\", case=False, na=False) \\\n",
    "        | value.str.contains(\"not available\", case=False, na=False) \\\n",
    "        | value.str.contains(\"none\", case=False, na=False)\n",
    "\n",
    "combined_dataset.loc[IsNotAvailable(combined_dataset[\"comparison\"]), \"comparison\"] = \"None\"\n",
    "\n",
    "combined_dataset[\"indication\"] = combined_dataset[\"indication\"].fillna(\"None\")\n",
    "combined_dataset[\"findings\"] = combined_dataset[\"findings\"].fillna(\"None\")\n",
    "combined_dataset[\"impression\"] = combined_dataset[\"impression\"].fillna(\"None\")\n",
    "combined_dataset[\"comparison\"] = combined_dataset[\"comparison\"].fillna(\"None\")\n",
    "combined_dataset[\"report\"] = combined_dataset[\"findings\"]\n",
    "# combined_dataset[\"report\"] = (\n",
    "#     \"Indication: \" + combined_dataset[\"indication\"].astype(str) + \"\\n\"\n",
    "#     + \"Findings: \" + combined_dataset[\"findings\"].astype(str) + \"\\n\"\n",
    "#     + \"Impression: \" + combined_dataset[\"impression\"].astype(str) + \"\\n\"\n",
    "#     + \"Comparison: \" + combined_dataset[\"comparison\"].astype(str)\n",
    "# )\n",
    "\n",
    "combined_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7f9581-a1e8-48dd-ae44-280b2279eb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>filename</th>\n",
       "      <th>projection</th>\n",
       "      <th>MeSH</th>\n",
       "      <th>Problems</th>\n",
       "      <th>image</th>\n",
       "      <th>indication</th>\n",
       "      <th>comparison</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>None</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>None</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n",
       "      <td>Cardiomegaly;Pulmonary Artery</td>\n",
       "      <td>Chest, 2 views, frontal and lateral</td>\n",
       "      <td>Preop bariatric surgery.</td>\n",
       "      <td>None</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>No acute pulmonary findings.</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n",
       "      <td>Cardiomegaly;Pulmonary Artery</td>\n",
       "      <td>Chest, 2 views, frontal and lateral</td>\n",
       "      <td>Preop bariatric surgery.</td>\n",
       "      <td>None</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>No acute pulmonary findings.</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>rib pain after a XXXX, XXXX XXXX steps this XX...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No displaced rib fractures, pneumothorax, or p...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                filename projection  \\\n",
       "0    1  1_IM-0001-4001.dcm.png    Frontal   \n",
       "1    1  1_IM-0001-3001.dcm.png    Lateral   \n",
       "2    2  2_IM-0652-1001.dcm.png    Frontal   \n",
       "3    2  2_IM-0652-2001.dcm.png    Lateral   \n",
       "4    3  3_IM-1384-1001.dcm.png    Frontal   \n",
       "\n",
       "                                                MeSH  \\\n",
       "0                                             normal   \n",
       "1                                             normal   \n",
       "2  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n",
       "3  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n",
       "4                                             normal   \n",
       "\n",
       "                        Problems                                image  \\\n",
       "0                         normal            Xray Chest PA and Lateral   \n",
       "1                         normal            Xray Chest PA and Lateral   \n",
       "2  Cardiomegaly;Pulmonary Artery  Chest, 2 views, frontal and lateral   \n",
       "3  Cardiomegaly;Pulmonary Artery  Chest, 2 views, frontal and lateral   \n",
       "4                         normal            Xray Chest PA and Lateral   \n",
       "\n",
       "                                          indication comparison  \\\n",
       "0                                   Positive TB test       None   \n",
       "1                                   Positive TB test       None   \n",
       "2                           Preop bariatric surgery.       None   \n",
       "3                           Preop bariatric surgery.       None   \n",
       "4  rib pain after a XXXX, XXXX XXXX steps this XX...       None   \n",
       "\n",
       "                                            findings  \\\n",
       "0  The cardiac silhouette and mediastinum size ar...   \n",
       "1  The cardiac silhouette and mediastinum size ar...   \n",
       "2  Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "3  Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "4                                               None   \n",
       "\n",
       "                                          impression  \\\n",
       "0                               Normal chest x-XXXX.   \n",
       "1                               Normal chest x-XXXX.   \n",
       "2                       No acute pulmonary findings.   \n",
       "3                       No acute pulmonary findings.   \n",
       "4  No displaced rib fractures, pneumothorax, or p...   \n",
       "\n",
       "                                              report  \n",
       "0  The cardiac silhouette and mediastinum size ar...  \n",
       "1  The cardiac silhouette and mediastinum size ar...  \n",
       "2  Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "3  Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ef11ed-b39c-4966-9fc6-142d4f233878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.\n",
      "-----\n",
      "The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.\n",
      "-----\n",
      "Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX.\n",
      "-----\n",
      "Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX.\n",
      "-----\n",
      "None\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for r in combined_dataset[\"report\"].head(5).to_list():\n",
    "    print(r)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee42af7-566e-4d47-a5b2-70896db8adae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>3997_IM-2048-1002.dcm.png</td>\n",
       "      <td>Heart size within normal limits. Small, nodula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7462</th>\n",
       "      <td>3998_IM-2048-1001.dcm.png</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>3998_IM-2048-1002.dcm.png</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>3999_IM-2049-1001.dcm.png</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>3999_IM-2049-2001.dcm.png</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7466 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "0        1_IM-0001-4001.dcm.png   \n",
       "1        1_IM-0001-3001.dcm.png   \n",
       "2        2_IM-0652-1001.dcm.png   \n",
       "3        2_IM-0652-2001.dcm.png   \n",
       "4        3_IM-1384-1001.dcm.png   \n",
       "...                         ...   \n",
       "7461  3997_IM-2048-1002.dcm.png   \n",
       "7462  3998_IM-2048-1001.dcm.png   \n",
       "7463  3998_IM-2048-1002.dcm.png   \n",
       "7464  3999_IM-2049-1001.dcm.png   \n",
       "7465  3999_IM-2049-2001.dcm.png   \n",
       "\n",
       "                                                 report  \n",
       "0     The cardiac silhouette and mediastinum size ar...  \n",
       "1     The cardiac silhouette and mediastinum size ar...  \n",
       "2     Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "3     Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "4                                                  None  \n",
       "...                                                 ...  \n",
       "7461  Heart size within normal limits. Small, nodula...  \n",
       "7462                                               None  \n",
       "7463                                               None  \n",
       "7464                                               None  \n",
       "7465                                               None  \n",
       "\n",
       "[7466 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dataset = combined_dataset.loc[:, (\"filename\", \"report\")]\n",
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57bdc739-7e88-4d32-88e0-d635556ec82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5972, 11), Test shape: (747, 11), Valid shape: (747, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(combined_dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "test_df, valid_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}, Valid shape: {valid_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e60acd-7357-4fcc-8f06-4cb39efc1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from timm import create_model, list_models\n",
    "\n",
    "sample_tfms = [\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.ColorJitter(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "]\n",
    "\n",
    "train_tfms = A.Compose([\n",
    "    *sample_tfms,\n",
    "    A.Resize(224,224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_tfms = A.Compose([\n",
    "    A.Resize(224,224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e70f10d-2b7f-405d-98b6-514f9700bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageReportDataset(Dataset):\n",
    "    def __init__(self, dataset, img_dir, tokenizer, transform=None):\n",
    "        self.data = dataset\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.loc[idx, \"filename\"])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        image = numpy.array(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        report = self.data.loc[idx, \"report\"] + \"<|endoftext|>\"\n",
    "        # inputs = self.tokenizer(report, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        # return image, inputs[\"input_ids\"].squeeze(), inputs[\"attention_mask\"].squeeze()\n",
    "        inputs = self.tokenizer(report, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "        labels[:-1] = input_ids[1:]\n",
    "        return image, input_ids, labels\n",
    "        # return image, inputs[\"input_ids\"].squeeze(), inputs[\"attention_mask\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b48e0a-47d6-4141-ab0f-f75572fddafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torchvision import transforms\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image = [i[0] for i in batch]\n",
    "    input_ids = [i[1] for i in batch]\n",
    "    labels = [i[2] for i in batch]\n",
    "    image = torch.stack(image, dim=0)\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    \n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": labels},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    \n",
    "    mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    labels[mask==0] = -100\n",
    "    return image, input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e57cc9-2676-4a96-b913-d435abcac647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension should be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        # q,k,v shape individually: batch_size x seq_len x embed_dim\n",
    "        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "        q = q.view(b, t, self.n_heads,self.head_size).permute(0, 2, 1, 3) # batch x n_heads x seq_len x head_dim\n",
    "        k = k.view(b, t, self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, t, self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        qk_t = qk_t.masked_fill(self.mask[:, :, :t, :t] == 0, float(\"-inf\"))\n",
    "        qk_t = F.softmax(qk_t, dim=-1)\n",
    "        weights = self.attn_dropout(qk_t)\n",
    "\n",
    "        attention = weights @ v # batch x n_heads x t x head_size\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c) # batch x t x embed_dim\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ba63448-741a-4403-b211-3e00ec0f703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension by be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "\n",
    "        self.q = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b, t, c = q.shape\n",
    "\n",
    "        q = self.q(q)\n",
    "        k = self.k(k)\n",
    "        v = self.v(v)\n",
    "\n",
    "        q = q.view(b, q.size(1), self.n_heads,self.head_size).permute(0, 2, 1, 3) # batch x n_heads x seq_len x head_dim\n",
    "        k = k.view(b, k.size(1), self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, v.size(1), self.n_heads,self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        qk_t = F.softmax(qk_t,dim=-1)\n",
    "        weights = self.attn_dropout(qk_t)\n",
    "\n",
    "        attention = weights @ v # batch x n_heads x t x head_size\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c) # batch x t x embed_dim\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f49162-e542-42ca-8e4d-a05231d23d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.mlp_ratio = config.mlp_ratio\n",
    "        self.mlp_dropout = config.mlp_dropout\n",
    "\n",
    "        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n",
    "        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(self.mlp_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf1011a-b533-4204-abf7-e5bc92fa1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ln_1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "        self.ln_3 = nn.LayerNorm(self.embed_dim)\n",
    "        self.cross_attn = GPT2CrossAttention(config)\n",
    "\n",
    "    def forward(self,x,enc_out):\n",
    "        x = x+self.attn(self.ln_1(x))\n",
    "        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out)\n",
    "        x = x+self.mlp(self.ln_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173729f5-8977-472c-baf6-74adf0b3b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        vit = create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = vit.cls_token\n",
    "        embed_len = num_patches + vit.num_prefix_tokens\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = nn.Dropout(p=0.)\n",
    "\n",
    "        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n",
    "            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n",
    "            drop = nn.Dropout(config.emb_dropout),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n",
    "            ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def _pos_embed(self,x):\n",
    "        pos_embed = self.pos_embed\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + pos_embed\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def pretrained_layers_trainable(self,trainable=False):\n",
    "        layers = [\n",
    "            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n",
    "            self.transformer.wte, self.transformer.wpe,\n",
    "            self.transformer.ln_f, self.lm_head\n",
    "        ]\n",
    "        gpt_layers = [[\n",
    "            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n",
    "            self.transformer.h[i].attn,self.transformer.h[i].mlp\n",
    "        ] for i in range(self.config.depth)]\n",
    "        for l in gpt_layers:\n",
    "            layers.extend(l)\n",
    "\n",
    "        for layer in layers:\n",
    "            if not isinstance(layer,nn.Parameter):\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = trainable\n",
    "            else:\n",
    "                layer.requires_grad = trainable\n",
    "\n",
    "        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n",
    "        print(f'{total_frozen_params=}')\n",
    "\n",
    "    def unfreeze_gpt_layers(self,):\n",
    "        gpt_layers = [[\n",
    "            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n",
    "            self.transformer.h[i].attn,self.transformer.h[i].mlp\n",
    "        ] for i in range(self.config.depth)]\n",
    "        flatten = []\n",
    "        for l in gpt_layers:\n",
    "            flatten.extend(l)\n",
    "\n",
    "        for layer in flatten:\n",
    "            if not isinstance(layer,nn.Parameter):\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                layer.requires_grad = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(self, config):\n",
    "        model = VisionGPT2Model(config)\n",
    "        sd = model.state_dict()\n",
    "        keys = sd.keys()\n",
    "        ignore_matches = [\"blocks.\", \"cross_attn.\", \"ln_3\", \"cls_token\", \"pos_embed\", \"patch_embed.\", \".attn.mask\"]\n",
    "        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n",
    "        gpt_keys = [key for key in keys if key not in vit_keys]\n",
    "\n",
    "        gpt2_small = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        sd_hf = gpt2_small.state_dict()\n",
    "        hf_keys = sd_hf.keys()\n",
    "        hf_keys = [k for k in hf_keys if not k.endswith(\".attn.masked_bias\")]\n",
    "        hf_keys = [k for k in hf_keys if not k.endswith(\".attn.bias\")]\n",
    "        transposed = [\"attn.c_attn.weight\", \"attn.c_proj.weight\", \"mlp.c_fc.weight\", \"mlp.c_proj.weight\"]\n",
    "\n",
    "        for k in hf_keys:\n",
    "            if any(match in k for match in ignore_matches):\n",
    "                continue\n",
    "                \n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        model.load_state_dict(sd)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, image, input_ids, labels=None):\n",
    "\n",
    "        image = self.patch_embed(image)\n",
    "        image = self._pos_embed(image)\n",
    "\n",
    "        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n",
    "        pos_embs = torch.arange(0, input_ids.size(1)).to(input_ids.device)\n",
    "        positional_embeddings = self.transformer.wpe(pos_embs)\n",
    "        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n",
    "\n",
    "        for i in range(self.config.depth):\n",
    "            image = self.blocks[i](image)\n",
    "            input_ids = self.transformer.h[i](input_ids, image)\n",
    "\n",
    "        input_ids = self.transformer.ln_f(input_ids)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits = self.lm_head(input_ids)\n",
    "            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n",
    "            return loss\n",
    "\n",
    "        lm_logits = self.lm_head(input_ids[:, [-1], :])\n",
    "        return lm_logits\n",
    "\n",
    "    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            out = self(image,sequence)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            probs = F.softmax(out, dim=-1)\n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sequence = torch.cat([sequence,next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return sequence.cpu().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "facc39c8-cc4b-49ea-93b0-5b2a8471dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_config, train_config, dls):\n",
    "        self.train_config = train_config\n",
    "        self.model_config = model_config\n",
    "        self.device = self.train_config.device\n",
    "\n",
    "        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        self.model.pretrained_layers_trainable(trainable=False)\n",
    "\n",
    "        print(f\"Trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}\")\n",
    "\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.scaler = GradScaler(self.device)\n",
    "\n",
    "        self.train_dl, self.val_dl = dls\n",
    "\n",
    "        total_steps = len(self.train_dl)\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n",
    "        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optim,\n",
    "            max_lr=self.train_config.lr,\n",
    "            epochs=self.train_config.epochs,\n",
    "            steps_per_epoch=total_steps\n",
    "        )\n",
    "\n",
    "#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "\n",
    "        self.metrics = pandas.DataFrame()\n",
    "        self.metrics[[\"train_loss\", \"train_perplexity\", \"val_loss\", \"val_perplexity\"]] = None\n",
    "\n",
    "        self.gen_tfms = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def save_model(self,):\n",
    "        self.train_config.model_path.mkdir(exist_ok=True)\n",
    "        sd = self.model.state_dict()\n",
    "        torch.save(self.model, self.train_config.model_path/\"captioner.pt\")\n",
    "\n",
    "    def load_best_model(self,):\n",
    "        sd = torch.load(self.train_config.model_path/\"captioner.pt\", weights_only=False)\n",
    "        self.model.load_state_dict(sd)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        prog = tqdm(self.train_dl, total=len(self.train_dl))\n",
    "        running_loss = 0.\n",
    "\n",
    "        for image, input_ids, labels in prog:\n",
    "            with autocast(self.device):\n",
    "                image = image.to(self.device)\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                loss = self.model(image,input_ids,labels)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optim)\n",
    "                self.scaler.update()\n",
    "                self.sched.step()\n",
    "                self.optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                prog.set_description(f'train loss: {loss.item():.3f}')\n",
    "\n",
    "            del image, input_ids, labels, loss\n",
    "        \n",
    "        print()\n",
    "\n",
    "        train_loss = running_loss / len(self.train_dl)\n",
    "        train_pxp = numpy.exp(train_loss)\n",
    "\n",
    "        self.metrics.loc[epoch, [\"train_loss\", \"train_perplexity\"]] = (train_loss, train_pxp)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self, epoch):\n",
    "\n",
    "        prog = tqdm(self.val_dl, total=len(self.val_dl))\n",
    "\n",
    "        running_loss = 0.\n",
    "\n",
    "        for image, input_ids, labels in prog:\n",
    "\n",
    "            with autocast(self.device):\n",
    "                image = image.to(self.device)\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                loss = self.model(image,input_ids,labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                prog.set_description(f\"Valid loss: {loss.item():.3f}\")\n",
    "\n",
    "            del image, input_ids, labels, loss\n",
    "\n",
    "        print()\n",
    "\n",
    "        val_loss = running_loss / len(self.val_dl)\n",
    "        val_pxp = numpy.exp(val_loss)\n",
    "\n",
    "        self.metrics.loc[epoch, [\"val_loss\", \"val_perplexity\"]] = (val_loss,val_pxp)\n",
    "\n",
    "        return val_pxp\n",
    "\n",
    "    def clean(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def fit(self,):\n",
    "        best_pxp = 1e9\n",
    "        best_epoch = -1\n",
    "        prog = tqdm(range(self.train_config.epochs))\n",
    "\n",
    "        for epoch in prog:\n",
    "            if epoch == self.train_config.freeze_epochs_gpt:\n",
    "                self.model.unfreeze_gpt_layers()\n",
    "                print(\"Unfreezing GPT2 entirely...\")\n",
    "\n",
    "            if epoch == self.train_config.freeze_epochs_all:\n",
    "                self.model.pretrained_layers_trainable(trainable=True)\n",
    "\n",
    "            self.model.train()\n",
    "            prog.set_description(\"Training\")\n",
    "            self.train_one_epoch(epoch)\n",
    "            self.clean()\n",
    "\n",
    "            self.model.eval()\n",
    "            prog.set_description(\"Validating\")\n",
    "            pxp = self.valid_one_epoch(epoch)\n",
    "            self.clean()\n",
    "\n",
    "            print(self.metrics.tail(1))\n",
    "\n",
    "            if pxp < best_pxp:\n",
    "                best_pxp = pxp\n",
    "                best_epoch = epoch\n",
    "                print(\"Saving best model...\")\n",
    "                self.save_model()\n",
    "\n",
    "        return {\n",
    "            \"best_perplexity\": best_pxp,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_caption(self, image, max_tokens=50, temperature=1.0, deterministic=False):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "        image = numpy.array(image)\n",
    "        image = self.gen_tfms(image=image)[\"image\"]\n",
    "        image = image.unsqueeze(0).to(self.device)\n",
    "        sequence = torch.ones(1, 1).to(device=self.device).long() * self.tokenizer.bos_token_id\n",
    "\n",
    "        caption = self.model.generate(\n",
    "            image,\n",
    "            sequence,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n",
    "\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c31d38da-e540-4469-b91b-2cefeb2392c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "model_config = SimpleNamespace(\n",
    "    vocab_size = 50_257,\n",
    "    embed_dim = 768, # 768\n",
    "    num_heads = 12,\n",
    "    seq_len = 1024,\n",
    "    depth = 12,\n",
    "    attention_dropout = 0.1,\n",
    "    residual_dropout = 0.1,\n",
    "    mlp_ratio = 4,\n",
    "    mlp_dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    ")\n",
    "\n",
    "train_config = SimpleNamespace(\n",
    "    epochs = 5,\n",
    "    freeze_epochs_gpt = 1,\n",
    "    freeze_epochs_all = 2,\n",
    "    lr = 1e-4,\n",
    "    device = 'cuda',\n",
    "    model_path = Path('./training/multi_modal'),\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0124e9aa-8c85-4320-a9c9-994afb9459e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageReportDataset(train_df, images_folder, tokenizer, train_tfms)\n",
    "valid_dataset = ImageReportDataset(valid_df, images_folder, tokenizer, valid_tfms)\n",
    "test_dataset = ImageReportDataset(test_df, images_folder, tokenizer, valid_tfms)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=False,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=False,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff3f6b23-0cbb-4df5-8acf-a7c0829519c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frozen_params=210236928\n",
      "Trainable parameters: 28366848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d499adbdcd9643e3a693d2dbd0d3a168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a0955a1bc34a43966baf119e14f6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0d49c5824440c6b54221f9b8048e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  train_loss train_perplexity  val_loss val_perplexity\n",
      "0   8.665057      5796.772245  5.812038     334.299603\n",
      "Saving best model...\n",
      "Unfreezing GPT2 entirely...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79444aa57aaf4db99922d9106ce87699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258afc39db1d45909b4b8ec59b649f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  train_loss train_perplexity  val_loss val_perplexity\n",
      "1   3.367834        29.015617  1.855544       6.395175\n",
      "Saving best model...\n",
      "total_frozen_params=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3c1998c32b494ca2879d17f9d0d6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94a8f1505554fac8fb3c9b84dd118cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  train_loss train_perplexity  val_loss val_perplexity\n",
      "2   1.749689         5.752815  1.401026       4.059363\n",
      "Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c503cc7c3144d419a793221fba188ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b28cc995fb4d0da4f8257fb73f82a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  train_loss train_perplexity  val_loss val_perplexity\n",
      "3   1.439368         4.218028  1.270694       3.563325\n",
      "Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3de1e9fc56b4be39c60836e6bf8a358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(model_config, train_config, (train_dataloader, valid_dataloader))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m    132\u001b[39m prog.set_description(\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.clean()\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mTrainer.train_one_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m     63\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.model(image,input_ids,labels)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m.sched.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model_config, train_config, (train_dataloader, valid_dataloader))\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bfc02e1-ab0d-42ac-adbf-a7e43ee2f746",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.VisionGPT2Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.VisionGPT2Model])` or the `torch.serialization.safe_globals([__main__.VisionGPT2Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mTrainer.load_best_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_best_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     sd = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcaptioner.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.load_state_dict(sd)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\serialization.py:1524\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1516\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1517\u001b[39m                     opened_zipfile,\n\u001b[32m   1518\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1521\u001b[39m                     **pickle_load_args,\n\u001b[32m   1522\u001b[39m                 )\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1526\u001b[39m             opened_zipfile,\n\u001b[32m   1527\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m             **pickle_load_args,\n\u001b[32m   1531\u001b[39m         )\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.VisionGPT2Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.VisionGPT2Model])` or the `torch.serialization.safe_globals([__main__.VisionGPT2Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "trainer.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1475d97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\v-mziadeh\\AppData\\Local\\Temp\\ipykernel_52400\\450406559.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  pandas.read_csv(\"training\\multi_modal\\metrics.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_perplexity</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.132064</td>\n",
       "      <td>25136.198009</td>\n",
       "      <td>8.420802</td>\n",
       "      <td>4540.544581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.224047</td>\n",
       "      <td>504.742017</td>\n",
       "      <td>4.302428</td>\n",
       "      <td>73.878977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.132028</td>\n",
       "      <td>22.920412</td>\n",
       "      <td>2.013945</td>\n",
       "      <td>7.492819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.939077</td>\n",
       "      <td>6.952334</td>\n",
       "      <td>1.537005</td>\n",
       "      <td>4.650640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.532418</td>\n",
       "      <td>4.629359</td>\n",
       "      <td>1.272288</td>\n",
       "      <td>3.569008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.304426</td>\n",
       "      <td>3.685574</td>\n",
       "      <td>1.132638</td>\n",
       "      <td>3.103833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.143736</td>\n",
       "      <td>3.138472</td>\n",
       "      <td>1.027499</td>\n",
       "      <td>2.794068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.024340</td>\n",
       "      <td>2.785256</td>\n",
       "      <td>0.946582</td>\n",
       "      <td>2.576886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.927683</td>\n",
       "      <td>2.528643</td>\n",
       "      <td>0.891535</td>\n",
       "      <td>2.438871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.844745</td>\n",
       "      <td>2.327385</td>\n",
       "      <td>0.826734</td>\n",
       "      <td>2.285841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.767671</td>\n",
       "      <td>2.154741</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>2.181287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.700491</td>\n",
       "      <td>2.014742</td>\n",
       "      <td>0.737892</td>\n",
       "      <td>2.091522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.634430</td>\n",
       "      <td>1.885947</td>\n",
       "      <td>0.698708</td>\n",
       "      <td>2.011153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.575762</td>\n",
       "      <td>1.778485</td>\n",
       "      <td>0.677022</td>\n",
       "      <td>1.968009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.528166</td>\n",
       "      <td>1.695819</td>\n",
       "      <td>0.649875</td>\n",
       "      <td>1.915301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.487052</td>\n",
       "      <td>1.627511</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>1.880190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.456219</td>\n",
       "      <td>1.578096</td>\n",
       "      <td>0.618221</td>\n",
       "      <td>1.855625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.433538</td>\n",
       "      <td>1.542706</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>1.845972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.422319</td>\n",
       "      <td>1.525495</td>\n",
       "      <td>0.607741</td>\n",
       "      <td>1.836279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.415934</td>\n",
       "      <td>1.515786</td>\n",
       "      <td>0.607744</td>\n",
       "      <td>1.836285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  train_perplexity  val_loss  val_perplexity\n",
       "0    10.132064      25136.198009  8.420802     4540.544581\n",
       "1     6.224047        504.742017  4.302428       73.878977\n",
       "2     3.132028         22.920412  2.013945        7.492819\n",
       "3     1.939077          6.952334  1.537005        4.650640\n",
       "4     1.532418          4.629359  1.272288        3.569008\n",
       "5     1.304426          3.685574  1.132638        3.103833\n",
       "6     1.143736          3.138472  1.027499        2.794068\n",
       "7     1.024340          2.785256  0.946582        2.576886\n",
       "8     0.927683          2.528643  0.891535        2.438871\n",
       "9     0.844745          2.327385  0.826734        2.285841\n",
       "10    0.767671          2.154741  0.779915        2.181287\n",
       "11    0.700491          2.014742  0.737892        2.091522\n",
       "12    0.634430          1.885947  0.698708        2.011153\n",
       "13    0.575762          1.778485  0.677022        1.968009\n",
       "14    0.528166          1.695819  0.649875        1.915301\n",
       "15    0.487052          1.627511  0.631373        1.880190\n",
       "16    0.456219          1.578096  0.618221        1.855625\n",
       "17    0.433538          1.542706  0.613006        1.845972\n",
       "18    0.422319          1.525495  0.607741        1.836279\n",
       "19    0.415934          1.515786  0.607744        1.836285"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.read_csv(\"training\\multi_modal\\metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de0dc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionGPT2Model(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): GPT2CrossAttention(\n",
       "          (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = torch.load(\"training/multi_modal/captioner.pt\", weights_only=False)\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e13490fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardiomediastinal contour stable and within normal limits. Changes of prior CABG again noted. Normal pulmonary vascularity. Streaky bibasilar opacities decreased from previous, possibly subsegmental atelectasis and/or scar. No pneumothorax or pleural effusion demonstrated. Redemonstrated severe L1 XXXX fracture. Slight interval increase in XXXX loss of T11 and there is XXXX mild to moderate anterior XXXX loss of T10. Degenerative changes of the spine. Abdominal aortic stent.\n",
      "------------\n",
      " XXXX examination consists of frontal and lateral radiographs of the chest. The cardiomediastinal contours are within normal limits allowing for low lung volumes and patient rotation. There is posterolateral view of the patient body without significant change. Removal of XXXX opacities in the left lung base may represent atelectasis or scarring. No focal consolidation, pleural effusion, or pneumothorax identified. Dense fracture of the left hemidiaphragm is stable. Degenerative disease of the thoracic spine appears worse than prior exam.\n"
     ]
    }
   ],
   "source": [
    "def generate_caption(model, image, max_tokens=200, temperature=1.0, deterministic=False):\n",
    "    # model.eval()\n",
    "    gen_tfms = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image).convert('RGB')\n",
    "    image = numpy.array(image)\n",
    "    image = gen_tfms(image=image)['image']\n",
    "    image = image.unsqueeze(0).to(device)  # Move the input image tensor to the same device as the model\n",
    "    sequence = torch.ones(1, 1).long().to(device) * tokenizer.bos_token_id\n",
    "\n",
    "    caption = model.generate(\n",
    "        image,\n",
    "        sequence,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        deterministic=deterministic\n",
    "    )\n",
    "    caption = tokenizer.decode(caption.cpu().numpy(), skip_special_tokens=True)  # Move the generated caption back to CPU for decoding\n",
    "\n",
    "    return caption\n",
    "\n",
    "img_path = os.path.join(images_folder, test_df.loc[0, \"filename\"])\n",
    "generated_report = generate_caption(best_model, img_path)\n",
    "print(test_df.loc[0, \"report\"])\n",
    "print(\"------------\")\n",
    "print(generated_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23ef094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\v-mziadeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60f28cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate BLEU score\n",
    "def compute_bleu(reference_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Compute BLEU score between generated texts and references.\n",
    "    \n",
    "    :param reference_texts: List of lists of reference texts (for each generated report)\n",
    "    :param generated_texts: List of generated reports\n",
    "    :return: BLEU score\n",
    "    \"\"\"\n",
    "    references = [[ref.split()] for ref in reference_texts]  # List of list of reference tokens\n",
    "    candidates = [gen.split() for gen in generated_texts]   # List of list of generated tokens\n",
    "    bleu_score = corpus_bleu(references, candidates)\n",
    "    return bleu_score\n",
    "\n",
    "# Function to calculate ROUGE score\n",
    "def compute_rouge(reference_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Compute ROUGE score between generated texts and references.\n",
    "    \n",
    "    :param reference_texts: List of reference reports\n",
    "    :param generated_texts: List of generated reports\n",
    "    :return: ROUGE score\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    \n",
    "    for reference, generated in zip(reference_texts, generated_texts):\n",
    "        scores = scorer.score(reference, generated)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(scores[key].fmeasure)\n",
    "    \n",
    "    avg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\n",
    "    avg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\n",
    "    avg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n",
    "    \n",
    "    return avg_rouge1, avg_rouge2, avg_rougeL\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, folder_path, eval_set):\n",
    "    generated_reports = []\n",
    "    reference_reports = []\n",
    "\n",
    "    print(f\"Starting evaluation for model using {len(eval_set)} items\")\n",
    "    for idx in range(len(eval_set)):\n",
    "        # Generate report for each image\n",
    "        generated_report = generate_caption(model, os.path.join(folder_path, eval_set.loc[idx, \"filename\"]))\n",
    "        reference_report = eval_set.loc[idx, \"report\"]\n",
    "        \n",
    "        generated_reports.append(generated_report)\n",
    "        reference_reports.append(reference_report)\n",
    "\n",
    "        print(f\"Generated report {idx + 1}/{len(eval_set)}    \\r\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # Compute BLEU\n",
    "    bleu_score = compute_bleu(reference_reports, generated_reports)\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    rouge1, rouge2, rougeL = compute_rouge(reference_reports, generated_reports)\n",
    "    print(f\"ROUGE-1: {rouge1:.4f}, ROUGE-2: {rouge2:.4f}, ROUGE-L: {rougeL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c41a6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for model using 747 items\n",
      "Generated report 747/747    \n",
      "BLEU Score: 0.0458\n",
      "ROUGE-1: 0.2948, ROUGE-2: 0.0945, ROUGE-L: 0.2019\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(best_model, images_folder, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a92fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
