{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ef2bad8-a350-4b08-8300-e91d01ff0ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\v-mziadeh\\.cache\\kagglehub\\datasets\\raddar\\chest-xrays-indiana-university\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64b7996e-e51c-43d6-9a50-353412da42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "2.7.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a7e9aba-ddd7-456e-aa14-e16d20e3980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7466 entries, 0 to 7465\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   uid         7466 non-null   int64 \n",
      " 1   filename    7466 non-null   object\n",
      " 2   projection  7466 non-null   object\n",
      " 3   MeSH        7466 non-null   object\n",
      " 4   Problems    7466 non-null   object\n",
      " 5   image       7466 non-null   object\n",
      " 6   indication  7466 non-null   object\n",
      " 7   comparison  7466 non-null   object\n",
      " 8   findings    7466 non-null   object\n",
      " 9   impression  7466 non-null   object\n",
      " 10  report      7466 non-null   object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 641.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "dataset_folder = path\n",
    "images_folder = dataset_folder + \"/images/images_normalized\"\n",
    "projections = pandas.read_csv(dataset_folder + \"/indiana_projections.csv\")\n",
    "reports = pandas.read_csv(dataset_folder + \"/indiana_reports.csv\")\n",
    "\n",
    "combined_dataset = projections.merge(reports, on=\"uid\", how=\"inner\")\n",
    "\n",
    "def IsNotAvailable(value):\n",
    "    return value.str.contains(\"unavailable\", case=False, na=False) \\\n",
    "        | value.str.contains(\"not available\", case=False, na=False) \\\n",
    "        | value.str.contains(\"none\", case=False, na=False)\n",
    "\n",
    "combined_dataset.loc[IsNotAvailable(combined_dataset[\"comparison\"]), \"comparison\"] = \"None\"\n",
    "\n",
    "combined_dataset[\"indication\"] = combined_dataset[\"indication\"].fillna(\"None\")\n",
    "combined_dataset[\"findings\"] = combined_dataset[\"findings\"].fillna(\"None\")\n",
    "combined_dataset[\"impression\"] = combined_dataset[\"impression\"].fillna(\"None\")\n",
    "combined_dataset[\"comparison\"] = combined_dataset[\"comparison\"].fillna(\"None\")\n",
    "combined_dataset[\"report\"] = combined_dataset[\"findings\"]\n",
    "combined_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea7f9581-a1e8-48dd-ae44-280b2279eb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>filename</th>\n",
       "      <th>projection</th>\n",
       "      <th>MeSH</th>\n",
       "      <th>Problems</th>\n",
       "      <th>image</th>\n",
       "      <th>indication</th>\n",
       "      <th>comparison</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>None</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>None</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n",
       "      <td>Cardiomegaly;Pulmonary Artery</td>\n",
       "      <td>Chest, 2 views, frontal and lateral</td>\n",
       "      <td>Preop bariatric surgery.</td>\n",
       "      <td>None</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>No acute pulmonary findings.</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Cardiomegaly/borderline;Pulmonary Artery/enlarged</td>\n",
       "      <td>Cardiomegaly;Pulmonary Artery</td>\n",
       "      <td>Chest, 2 views, frontal and lateral</td>\n",
       "      <td>Preop bariatric surgery.</td>\n",
       "      <td>None</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>No acute pulmonary findings.</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>rib pain after a XXXX, XXXX XXXX steps this XX...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No displaced rib fractures, pneumothorax, or p...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid                filename projection  \\\n",
       "0    1  1_IM-0001-4001.dcm.png    Frontal   \n",
       "1    1  1_IM-0001-3001.dcm.png    Lateral   \n",
       "2    2  2_IM-0652-1001.dcm.png    Frontal   \n",
       "3    2  2_IM-0652-2001.dcm.png    Lateral   \n",
       "4    3  3_IM-1384-1001.dcm.png    Frontal   \n",
       "\n",
       "                                                MeSH  \\\n",
       "0                                             normal   \n",
       "1                                             normal   \n",
       "2  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n",
       "3  Cardiomegaly/borderline;Pulmonary Artery/enlarged   \n",
       "4                                             normal   \n",
       "\n",
       "                        Problems                                image  \\\n",
       "0                         normal            Xray Chest PA and Lateral   \n",
       "1                         normal            Xray Chest PA and Lateral   \n",
       "2  Cardiomegaly;Pulmonary Artery  Chest, 2 views, frontal and lateral   \n",
       "3  Cardiomegaly;Pulmonary Artery  Chest, 2 views, frontal and lateral   \n",
       "4                         normal            Xray Chest PA and Lateral   \n",
       "\n",
       "                                          indication comparison  \\\n",
       "0                                   Positive TB test       None   \n",
       "1                                   Positive TB test       None   \n",
       "2                           Preop bariatric surgery.       None   \n",
       "3                           Preop bariatric surgery.       None   \n",
       "4  rib pain after a XXXX, XXXX XXXX steps this XX...       None   \n",
       "\n",
       "                                            findings  \\\n",
       "0  The cardiac silhouette and mediastinum size ar...   \n",
       "1  The cardiac silhouette and mediastinum size ar...   \n",
       "2  Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "3  Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "4                                               None   \n",
       "\n",
       "                                          impression  \\\n",
       "0                               Normal chest x-XXXX.   \n",
       "1                               Normal chest x-XXXX.   \n",
       "2                       No acute pulmonary findings.   \n",
       "3                       No acute pulmonary findings.   \n",
       "4  No displaced rib fractures, pneumothorax, or p...   \n",
       "\n",
       "                                              report  \n",
       "0  The cardiac silhouette and mediastinum size ar...  \n",
       "1  The cardiac silhouette and mediastinum size ar...  \n",
       "2  Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "3  Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9ef11ed-b39c-4966-9fc6-142d4f233878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.\n",
      "-----\n",
      "The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax.\n",
      "-----\n",
      "Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX.\n",
      "-----\n",
      "Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX.\n",
      "-----\n",
      "None\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for r in combined_dataset[\"report\"].head(5).to_list():\n",
    "    print(r)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aee42af7-566e-4d47-a5b2-70896db8adae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>projection</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>3997_IM-2048-1002.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Heart size within normal limits. Small, nodula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7462</th>\n",
       "      <td>3998_IM-2048-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>3998_IM-2048-1002.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>3999_IM-2049-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>3999_IM-2049-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7466 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename projection  \\\n",
       "0        1_IM-0001-4001.dcm.png    Frontal   \n",
       "1        1_IM-0001-3001.dcm.png    Lateral   \n",
       "2        2_IM-0652-1001.dcm.png    Frontal   \n",
       "3        2_IM-0652-2001.dcm.png    Lateral   \n",
       "4        3_IM-1384-1001.dcm.png    Frontal   \n",
       "...                         ...        ...   \n",
       "7461  3997_IM-2048-1002.dcm.png    Lateral   \n",
       "7462  3998_IM-2048-1001.dcm.png    Frontal   \n",
       "7463  3998_IM-2048-1002.dcm.png    Lateral   \n",
       "7464  3999_IM-2049-1001.dcm.png    Frontal   \n",
       "7465  3999_IM-2049-2001.dcm.png    Lateral   \n",
       "\n",
       "                                                 report  \n",
       "0     The cardiac silhouette and mediastinum size ar...  \n",
       "1     The cardiac silhouette and mediastinum size ar...  \n",
       "2     Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "3     Borderline cardiomegaly. Midline sternotomy XX...  \n",
       "4                                                  None  \n",
       "...                                                 ...  \n",
       "7461  Heart size within normal limits. Small, nodula...  \n",
       "7462                                               None  \n",
       "7463                                               None  \n",
       "7464                                               None  \n",
       "7465                                               None  \n",
       "\n",
       "[7466 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dataset = combined_dataset.loc[:, (\"filename\", \"projection\", \"report\")]\n",
    "reduced_dataset[\"report\"] = reduced_dataset[\"report\"].fillna(\"None\")\n",
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9c6ee415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchxrayvision as xrv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import skimage\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "\n",
    "class XRayVisionKeywordsExtractor():\n",
    "    def __init__(self, model_name=\"densenet121-res224-all\"):\n",
    "        self.model = xrv.models.DenseNet(weights=model_name)\n",
    "        self.transform = transforms.Compose([xrv.datasets.XRayCenterCrop(), xrv.datasets.XRayResizer(224)])\n",
    "\n",
    "    def extract(self, images):\n",
    "        keywords = []\n",
    "        for filepath in images:\n",
    "            img = skimage.io.imread(filepath)\n",
    "            img = xrv.datasets.normalize(img, 255)\n",
    "            img = numpy.expand_dims(img, axis=0)  # Add channel dimension\n",
    "\n",
    "            img = self.transform(img)\n",
    "            img = torch.from_numpy(img)\n",
    "\n",
    "            outputs = self.model(img[None, ...])\n",
    "            predicted_pathologies = pd.DataFrame(zip(self.model.pathologies, outputs[0].detach().numpy()), columns=[\"Pathology\", \"Score\"])\n",
    "            top_pathologies = predicted_pathologies.loc[predicted_pathologies[\"Score\"] >= 0.5].sort_values(by=\"Score\", ascending=False)\n",
    "            keywords.append(top_pathologies[\"Pathology\"].values)\n",
    "\n",
    "        return keywords\n",
    "\n",
    "# xrv_keywords_extractor = XRayVisionKeywordsExtractor()\n",
    "# xrv_keywords = xrv_keywords_extractor.extract(reduced_dataset[\"filename\"].head(2).to_list())\n",
    "# xrv_keywords\n",
    "# xrv_model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "\n",
    "# xrv_transform = transforms.Compose([xrv.datasets.XRayCenterCrop(), xrv.datasets.XRayResizer(224)])\n",
    "\n",
    "# def add_keywords(images):\n",
    "#     keywords = []\n",
    "#     total = len(images)\n",
    "#     count = 0\n",
    "#     for filename in images:\n",
    "#         img = skimage.io.imread(os.path.join(images_folder, filename))\n",
    "#         img = xrv.datasets.normalize(img, 255)\n",
    "#         img = numpy.expand_dims(img, axis=0)  # Add channel dimension\n",
    "\n",
    "#         img = xrv_transform(img)\n",
    "#         img = torch.from_numpy(img)\n",
    "\n",
    "#         outputs = xrv_model(img[None, ...])\n",
    "#         predicted_pathologies = pandas.DataFrame(zip(xrv_model.pathologies, outputs[0].detach().numpy()), columns=[\"Pathology\", \"Score\"])\n",
    "#         top_pathologies = predicted_pathologies.loc[predicted_pathologies[\"Score\"] >= 0.5].sort_values(by=\"Score\", ascending=False)\n",
    "#         keywords.append(top_pathologies[\"Pathology\"].values)\n",
    "\n",
    "#         count += 1\n",
    "#         print(f\"Processed {count}/{total} images        \", end=\"\\r\")\n",
    "    \n",
    "#     return keywords\n",
    "\n",
    "# reduced_dataset[\"keywords\"] = add_keywords(reduced_dataset[\"filename\"].to_list())\n",
    "# reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b798943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>projection</th>\n",
       "      <th>report</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_IM-0001-4001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_IM-0001-3001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Lung Opacity, Pneumonia, Infiltration, Consoli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_IM-0652-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>Cardiomegaly, Fibrosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_IM-0652-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Borderline cardiomegaly. Midline sternotomy XX...</td>\n",
       "      <td>Lung Opacity, Infiltration, Atelectasis, Fract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_IM-1384-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>3997_IM-2048-1002.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>Heart size within normal limits. Small, nodula...</td>\n",
       "      <td>Lung Opacity, Infiltration, Enlarged Cardiomed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7462</th>\n",
       "      <td>3998_IM-2048-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>None</td>\n",
       "      <td>Fibrosis, Infiltration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>3998_IM-2048-1002.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>None</td>\n",
       "      <td>Lung Opacity, Atelectasis, Mass, Fracture, Fib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>3999_IM-2049-1001.dcm.png</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>3999_IM-2049-2001.dcm.png</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>None</td>\n",
       "      <td>Lung Opacity, Mass, Fibrosis, Atelectasis, Nod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7466 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename projection  \\\n",
       "0        1_IM-0001-4001.dcm.png    Frontal   \n",
       "1        1_IM-0001-3001.dcm.png    Lateral   \n",
       "2        2_IM-0652-1001.dcm.png    Frontal   \n",
       "3        2_IM-0652-2001.dcm.png    Lateral   \n",
       "4        3_IM-1384-1001.dcm.png    Frontal   \n",
       "...                         ...        ...   \n",
       "7461  3997_IM-2048-1002.dcm.png    Lateral   \n",
       "7462  3998_IM-2048-1001.dcm.png    Frontal   \n",
       "7463  3998_IM-2048-1002.dcm.png    Lateral   \n",
       "7464  3999_IM-2049-1001.dcm.png    Frontal   \n",
       "7465  3999_IM-2049-2001.dcm.png    Lateral   \n",
       "\n",
       "                                                 report  \\\n",
       "0     The cardiac silhouette and mediastinum size ar...   \n",
       "1     The cardiac silhouette and mediastinum size ar...   \n",
       "2     Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "3     Borderline cardiomegaly. Midline sternotomy XX...   \n",
       "4                                                  None   \n",
       "...                                                 ...   \n",
       "7461  Heart size within normal limits. Small, nodula...   \n",
       "7462                                               None   \n",
       "7463                                               None   \n",
       "7464                                               None   \n",
       "7465                                               None   \n",
       "\n",
       "                                               keywords  \n",
       "0                                                        \n",
       "1     Lung Opacity, Pneumonia, Infiltration, Consoli...  \n",
       "2                                Cardiomegaly, Fibrosis  \n",
       "3     Lung Opacity, Infiltration, Atelectasis, Fract...  \n",
       "4                                                        \n",
       "...                                                 ...  \n",
       "7461  Lung Opacity, Infiltration, Enlarged Cardiomed...  \n",
       "7462                             Fibrosis, Infiltration  \n",
       "7463  Lung Opacity, Atelectasis, Mass, Fracture, Fib...  \n",
       "7464                                                     \n",
       "7465  Lung Opacity, Mass, Fibrosis, Atelectasis, Nod...  \n",
       "\n",
       "[7466 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# reduced_dataset.to_csv(\"./reduced_dataset_with_keywords.csv\", index=False)\n",
    "reduced_dataset = pandas.read_csv(\"./reduced_dataset_with_keywords.csv\")\n",
    "reduced_dataset[\"report\"] = reduced_dataset[\"report\"].fillna(\"None\")\n",
    "reduced_dataset[\"keywords\"] = reduced_dataset[\"keywords\"].fillna(\"\").apply(lambda x: x.replace(\";\", \", \"))\n",
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "57bdc739-7e88-4d32-88e0-d635556ec82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5972, 4), Test shape: (747, 4), Valid shape: (747, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(reduced_dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "test_df, valid_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}, Valid shape: {valid_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "45e60acd-7357-4fcc-8f06-4cb39efc1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from timm import create_model, list_models\n",
    "\n",
    "sample_tfms = [\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.ColorJitter(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "]\n",
    "\n",
    "train_tfms = A.Compose([\n",
    "    *sample_tfms,\n",
    "    A.Resize(224,224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_tfms = A.Compose([\n",
    "    A.Resize(224,224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e70f10d-2b7f-405d-98b6-514f9700bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageReportWithKeywordsDataset(Dataset):\n",
    "    def __init__(self, dataset, img_dir, tokenizer, transform=None):\n",
    "        self.data = dataset\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.loc[idx, \"filename\"])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        image = numpy.array(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        keywords = self.data.loc[idx, \"keywords\"]\n",
    "        keywords = \"keywords: \" + self.data.loc[idx, \"keywords\"]\n",
    "        keywords = self.tokenizer(keywords, truncation=True, padding=\"max_length\", max_length=50, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        report = self.data.loc[idx, \"report\"] + \"<|endoftext|>\"\n",
    "        inputs = self.tokenizer(report, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "        labels[:-1] = input_ids[1:]\n",
    "        return image, input_ids, keywords, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "67b48e0a-47d6-4141-ab0f-f75572fddafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torchvision import transforms\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image = [i[0] for i in batch]\n",
    "    input_ids = [i[1] for i in batch]\n",
    "    keywords = [i[2] for i in batch]\n",
    "    labels = [i[3] for i in batch]\n",
    "    image = torch.stack(image, dim=0)\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    \n",
    "    keywords = tokenizer.pad(\n",
    "        {\"input_ids\": keywords},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": labels},\n",
    "        padding=\"longest\",\n",
    "        return_attention_mask=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids']\n",
    "    \n",
    "    mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    labels[mask==0] = -100\n",
    "    return image, input_ids, keywords, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "03e57cc9-2676-4a96-b913-d435abcac647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension should be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        # q,k,v shape individually: batch_size x seq_len x embed_dim\n",
    "        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)\n",
    "        q = q.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3) # batch x n_heads x seq_len x head_dim\n",
    "        k = k.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        qk_t = qk_t.masked_fill(self.mask[:, :, :t, :t] == 0, float(\"-inf\"))\n",
    "        qk_t = F.softmax(qk_t, dim=-1)\n",
    "        weights = self.attn_dropout(qk_t)\n",
    "\n",
    "        attention = weights @ v # batch x n_heads x t x head_size\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c) # batch x t x embed_dim\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ba63448-741a-4403-b211-3e00ec0f703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension must be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "\n",
    "        self.q = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b, t, c = q.shape\n",
    "\n",
    "        q = self.q(q)\n",
    "        k = self.k(k)\n",
    "        v = self.v(v)\n",
    "\n",
    "        q = q.view(b, q.size(1), self.n_heads, self.head_size).permute(0, 2, 1, 3) # batch x n_heads x seq_len x head_dim\n",
    "        k = k.view(b, k.size(1), self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, v.size(1), self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        qk_t = F.softmax(qk_t, dim=-1)\n",
    "        weights = self.attn_dropout(qk_t)\n",
    "\n",
    "        attention = weights @ v # batch x n_heads x t x head_size\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c) # batch x t x embed_dim\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "35f49162-e542-42ca-8e4d-a05231d23d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.mlp_ratio = config.mlp_ratio\n",
    "        self.mlp_dropout = config.mlp_dropout\n",
    "\n",
    "        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim * self.mlp_ratio)\n",
    "        self.c_proj = nn.Linear(self.embed_dim * self.mlp_ratio,self.embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(self.mlp_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9cf1011a-b533-4204-abf7-e5bc92fa1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ln_1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "        self.ln_3 = nn.LayerNorm(self.embed_dim)\n",
    "        self.cross_attn_1 = GPT2CrossAttention(config)\n",
    "        self.ln_4 = nn.LayerNorm(self.embed_dim)\n",
    "        self.cross_attn_2 = GPT2CrossAttention(config)\n",
    "\n",
    "    def forward(self, x, enc_out, keywords):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross_attn_1(self.ln_2(x), enc_out, enc_out)\n",
    "        x = x + self.cross_attn_2(self.ln_3(x), keywords, keywords)\n",
    "        x = x + self.mlp(self.ln_4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "173729f5-8977-472c-baf6-74adf0b3b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        vit = create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = vit.cls_token\n",
    "        embed_len = num_patches + vit.num_prefix_tokens\n",
    "        self.pos_embed = vit.pos_embed\n",
    "        self.pos_drop = nn.Dropout(p=0.)\n",
    "\n",
    "        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.embed_dim),\n",
    "            wpe = nn.Embedding(config.seq_len, config.embed_dim),\n",
    "            drop = nn.Dropout(config.emb_dropout),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n",
    "            ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def _pos_embed(self,x):\n",
    "        pos_embed = self.pos_embed\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + pos_embed\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def pretrained_layers_trainable(self, trainable=False):\n",
    "        layers = [\n",
    "            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n",
    "            self.transformer.wte, self.transformer.wpe,\n",
    "            self.transformer.ln_f, self.lm_head\n",
    "        ]\n",
    "\n",
    "        gpt_layers = [[\n",
    "            self.transformer.h[i].ln_1, self.transformer.h[i].ln_2,\n",
    "            self.transformer.h[i].attn, self.transformer.h[i].mlp\n",
    "        ] for i in range(self.config.depth)]\n",
    "\n",
    "        for l in gpt_layers:\n",
    "            layers.extend(l)\n",
    "\n",
    "        for layer in layers:\n",
    "            if not isinstance(layer, nn.Parameter):\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = trainable\n",
    "            else:\n",
    "                layer.requires_grad = trainable\n",
    "\n",
    "        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n",
    "        print(f'{total_frozen_params = }')\n",
    "\n",
    "    def unfreeze_gpt_layers(self,):\n",
    "        gpt_layers = [[\n",
    "            self.transformer.h[i].ln_1, self.transformer.h[i].ln_2,\n",
    "            self.transformer.h[i].attn, self.transformer.h[i].mlp\n",
    "        ] for i in range(self.config.depth)]\n",
    "        \n",
    "        flatten = []\n",
    "        for l in gpt_layers:\n",
    "            flatten.extend(l)\n",
    "\n",
    "        for layer in flatten:\n",
    "            if not isinstance(layer,nn.Parameter):\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                layer.requires_grad = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(self, config):\n",
    "        model = VisionGPT2Model(config)\n",
    "        sd = model.state_dict()\n",
    "        keys = sd.keys()\n",
    "        ignore_matches = [\"blocks.\", \"cross_attn.\", \"ln_3\", \"cls_token\", \"pos_embed\", \"patch_embed.\", \".attn.mask\"]\n",
    "        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n",
    "        gpt_keys = [key for key in keys if key not in vit_keys]\n",
    "\n",
    "        gpt2_small = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        sd_hf = gpt2_small.state_dict()\n",
    "        hf_keys = sd_hf.keys()\n",
    "        hf_keys = [k for k in hf_keys if not k.endswith(\".attn.masked_bias\")]\n",
    "        hf_keys = [k for k in hf_keys if not k.endswith(\".attn.bias\")]\n",
    "        transposed = [\"attn.c_attn.weight\", \"attn.c_proj.weight\", \"mlp.c_fc.weight\", \"mlp.c_proj.weight\"]\n",
    "\n",
    "        for k in hf_keys:\n",
    "            if any(match in k for match in ignore_matches):\n",
    "                continue\n",
    "                \n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        model.load_state_dict(sd)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, image, input_ids, keywords, labels=None):\n",
    "\n",
    "        image = self.patch_embed(image)\n",
    "        image = self._pos_embed(image)\n",
    "\n",
    "        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n",
    "        pos_embs = torch.arange(0, input_ids.size(1)).to(input_ids.device)\n",
    "        positional_embeddings = self.transformer.wpe(pos_embs)\n",
    "        input_ids = self.transformer.drop(token_embeddings + positional_embeddings)\n",
    "\n",
    "        keywords = self.transformer.wte(keywords).squeeze(1)  # batch x seq_len x embed_dim\n",
    "\n",
    "        for i in range(self.config.depth):\n",
    "            image = self.blocks[i](image)\n",
    "            input_ids = self.transformer.h[i](input_ids, image, keywords)\n",
    "\n",
    "        input_ids = self.transformer.ln_f(input_ids)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits = self.lm_head(input_ids)\n",
    "            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n",
    "            return loss\n",
    "\n",
    "        lm_logits = self.lm_head(input_ids[:, [-1], :])\n",
    "        return lm_logits\n",
    "\n",
    "    def generate(self, image, sequence, keywords, max_tokens=50, temperature=1.0, deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            out = self(image, sequence, keywords)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            probs = F.softmax(out, dim=-1)\n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            sequence = torch.cat([sequence,next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return sequence.cpu().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "facc39c8-cc4b-49ea-93b0-5b2a8471dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_config, train_config, dls):\n",
    "        self.train_config = train_config\n",
    "        self.model_config = model_config\n",
    "        self.device = self.train_config.device\n",
    "\n",
    "        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        self.model.pretrained_layers_trainable(trainable=False)\n",
    "\n",
    "        print(f\"Trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}\")\n",
    "\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.scaler = GradScaler(self.device)\n",
    "\n",
    "        self.train_dl, self.val_dl = dls\n",
    "\n",
    "        total_steps = len(self.train_dl)\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n",
    "        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optim,\n",
    "            max_lr=self.train_config.lr,\n",
    "            epochs=self.train_config.epochs,\n",
    "            steps_per_epoch=total_steps\n",
    "        )\n",
    "\n",
    "        self.metrics = pandas.DataFrame()\n",
    "        self.metrics[[\"train_loss\", \"train_perplexity\", \"val_loss\", \"val_perplexity\"]] = None\n",
    "\n",
    "        self.gen_tfms = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def save_model(self,):\n",
    "        self.train_config.model_path.mkdir(exist_ok=True)\n",
    "        sd = self.model.state_dict()\n",
    "        torch.save(self.model, self.train_config.model_path/\"captioner.pt\")\n",
    "\n",
    "    def load_best_model(self,):\n",
    "        sd = torch.load(self.train_config.model_path/\"captioner.pt\", weights_only=False)\n",
    "        self.model.load_state_dict(sd)\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        prog = tqdm(self.train_dl, total=len(self.train_dl))\n",
    "        running_loss = 0.\n",
    "\n",
    "        for image, input_ids, keywords, labels in prog:\n",
    "            with autocast(self.device):\n",
    "                image = image.to(self.device)\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                keywords = keywords.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                loss = self.model(image, input_ids, keywords, labels)\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optim)\n",
    "                self.scaler.update()\n",
    "                self.sched.step()\n",
    "                self.optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                prog.set_description(f'train loss: {loss.item():.3f}')\n",
    "\n",
    "            del image, input_ids, keywords, labels, loss\n",
    "        \n",
    "        print()\n",
    "\n",
    "        train_loss = running_loss / len(self.train_dl)\n",
    "        train_pxp = numpy.exp(train_loss)\n",
    "\n",
    "        self.metrics.loc[epoch, [\"train_loss\", \"train_perplexity\"]] = (train_loss, train_pxp)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self, epoch):\n",
    "\n",
    "        prog = tqdm(self.val_dl, total=len(self.val_dl))\n",
    "\n",
    "        running_loss = 0.\n",
    "\n",
    "        for image, input_ids, keywords, labels in prog:\n",
    "\n",
    "            with autocast(self.device):\n",
    "                image = image.to(self.device)\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                keywords = keywords.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                loss = self.model(image, input_ids, keywords, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                prog.set_description(f\"Valid loss: {loss.item():.3f}\")\n",
    "\n",
    "            del image, input_ids, keywords, labels, loss\n",
    "\n",
    "        print()\n",
    "\n",
    "        val_loss = running_loss / len(self.val_dl)\n",
    "        val_pxp = numpy.exp(val_loss)\n",
    "\n",
    "        self.metrics.loc[epoch, [\"val_loss\", \"val_perplexity\"]] = (val_loss,val_pxp)\n",
    "\n",
    "        return val_pxp\n",
    "\n",
    "    def clean(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def fit(self,):\n",
    "        best_pxp = 1e9\n",
    "        best_epoch = -1\n",
    "        prog = tqdm(range(self.train_config.epochs))\n",
    "\n",
    "        for epoch in prog:\n",
    "            if epoch == self.train_config.freeze_epochs_gpt:\n",
    "                self.model.unfreeze_gpt_layers()\n",
    "                print(\"Unfreezing GPT2 entirely...\")\n",
    "\n",
    "            if epoch == self.train_config.freeze_epochs_all:\n",
    "                self.model.pretrained_layers_trainable(trainable=True)\n",
    "\n",
    "            self.model.train()\n",
    "            prog.set_description(\"Training\")\n",
    "            self.train_one_epoch(epoch)\n",
    "            self.clean()\n",
    "\n",
    "            self.model.eval()\n",
    "            prog.set_description(\"Validating\")\n",
    "            pxp = self.valid_one_epoch(epoch)\n",
    "            self.clean()\n",
    "\n",
    "            print(self.metrics.tail(1))\n",
    "\n",
    "            if pxp < best_pxp:\n",
    "                best_pxp = pxp\n",
    "                best_epoch = epoch\n",
    "                print(\"Saving best model...\")\n",
    "                self.save_model()\n",
    "\n",
    "        return {\n",
    "            \"best_perplexity\": best_pxp,\n",
    "            \"best_epoch\": best_epoch\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_caption(self, image, max_tokens=50, temperature=1.0, deterministic=False):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "        image = numpy.array(image)\n",
    "        image = self.gen_tfms(image=image)[\"image\"]\n",
    "        image = image.unsqueeze(0).to(self.device)\n",
    "        sequence = torch.ones(1, 1).to(device=self.device).long() * self.tokenizer.bos_token_id\n",
    "\n",
    "        caption = self.model.generate(\n",
    "            image,\n",
    "            sequence,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n",
    "\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c31d38da-e540-4469-b91b-2cefeb2392c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "model_config = SimpleNamespace(\n",
    "    vocab_size = 50_257,\n",
    "    embed_dim = 768, # 768\n",
    "    num_heads = 12,\n",
    "    seq_len = 1024,\n",
    "    depth = 12,\n",
    "    attention_dropout = 0.1,\n",
    "    residual_dropout = 0.1,\n",
    "    mlp_ratio = 4,\n",
    "    mlp_dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    ")\n",
    "\n",
    "train_config = SimpleNamespace(\n",
    "    epochs = 1,\n",
    "    freeze_epochs_gpt = 1,\n",
    "    freeze_epochs_all = 2,\n",
    "    lr = 1e-4,\n",
    "    device = 'cuda',\n",
    "    model_path = Path('./training/multi_modal_withkeywords'),\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0124e9aa-8c85-4320-a9c9-994afb9459e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageReportWithKeywordsDataset(train_df, images_folder, tokenizer, train_tfms)\n",
    "valid_dataset = ImageReportWithKeywordsDataset(valid_df, images_folder, tokenizer, valid_tfms)\n",
    "test_dataset = ImageReportWithKeywordsDataset(test_df, images_folder, tokenizer, valid_tfms)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=False,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=False,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ff3f6b23-0cbb-4df5-8acf-a7c0829519c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frozen_params = 210236928\n",
      "Trainable parameters: 56733696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98241fdcb03d4c23bdc71b2158121f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c0723c80f04a738e00ee4849841279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[196]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(model_config, train_config, (train_dataloader, valid_dataloader))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train()\n\u001b[32m    132\u001b[39m prog.set_description(\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.clean()\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mTrainer.train_one_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m     60\u001b[39m labels = labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     62\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.model(image, input_ids, keywords, labels)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.step(\u001b[38;5;28mself\u001b[39m.optim)\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model_config, train_config, (train_dataloader, valid_dataloader))\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bfc02e1-ab0d-42ac-adbf-a7e43ee2f746",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.VisionGPT2Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.VisionGPT2Model])` or the `torch.serialization.safe_globals([__main__.VisionGPT2Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mTrainer.load_best_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_best_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     sd = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcaptioner.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.load_state_dict(sd)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torch\\serialization.py:1524\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1516\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1517\u001b[39m                     opened_zipfile,\n\u001b[32m   1518\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1521\u001b[39m                     **pickle_load_args,\n\u001b[32m   1522\u001b[39m                 )\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1526\u001b[39m             opened_zipfile,\n\u001b[32m   1527\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m             **pickle_load_args,\n\u001b[32m   1531\u001b[39m         )\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.VisionGPT2Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.VisionGPT2Model])` or the `torch.serialization.safe_globals([__main__.VisionGPT2Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "trainer.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1475d97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\v-mziadeh\\AppData\\Local\\Temp\\ipykernel_52400\\450406559.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  pandas.read_csv(\"training\\multi_modal\\metrics.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_perplexity</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.132064</td>\n",
       "      <td>25136.198009</td>\n",
       "      <td>8.420802</td>\n",
       "      <td>4540.544581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.224047</td>\n",
       "      <td>504.742017</td>\n",
       "      <td>4.302428</td>\n",
       "      <td>73.878977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.132028</td>\n",
       "      <td>22.920412</td>\n",
       "      <td>2.013945</td>\n",
       "      <td>7.492819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.939077</td>\n",
       "      <td>6.952334</td>\n",
       "      <td>1.537005</td>\n",
       "      <td>4.650640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.532418</td>\n",
       "      <td>4.629359</td>\n",
       "      <td>1.272288</td>\n",
       "      <td>3.569008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.304426</td>\n",
       "      <td>3.685574</td>\n",
       "      <td>1.132638</td>\n",
       "      <td>3.103833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.143736</td>\n",
       "      <td>3.138472</td>\n",
       "      <td>1.027499</td>\n",
       "      <td>2.794068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.024340</td>\n",
       "      <td>2.785256</td>\n",
       "      <td>0.946582</td>\n",
       "      <td>2.576886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.927683</td>\n",
       "      <td>2.528643</td>\n",
       "      <td>0.891535</td>\n",
       "      <td>2.438871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.844745</td>\n",
       "      <td>2.327385</td>\n",
       "      <td>0.826734</td>\n",
       "      <td>2.285841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.767671</td>\n",
       "      <td>2.154741</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>2.181287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.700491</td>\n",
       "      <td>2.014742</td>\n",
       "      <td>0.737892</td>\n",
       "      <td>2.091522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.634430</td>\n",
       "      <td>1.885947</td>\n",
       "      <td>0.698708</td>\n",
       "      <td>2.011153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.575762</td>\n",
       "      <td>1.778485</td>\n",
       "      <td>0.677022</td>\n",
       "      <td>1.968009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.528166</td>\n",
       "      <td>1.695819</td>\n",
       "      <td>0.649875</td>\n",
       "      <td>1.915301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.487052</td>\n",
       "      <td>1.627511</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>1.880190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.456219</td>\n",
       "      <td>1.578096</td>\n",
       "      <td>0.618221</td>\n",
       "      <td>1.855625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.433538</td>\n",
       "      <td>1.542706</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>1.845972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.422319</td>\n",
       "      <td>1.525495</td>\n",
       "      <td>0.607741</td>\n",
       "      <td>1.836279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.415934</td>\n",
       "      <td>1.515786</td>\n",
       "      <td>0.607744</td>\n",
       "      <td>1.836285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  train_perplexity  val_loss  val_perplexity\n",
       "0    10.132064      25136.198009  8.420802     4540.544581\n",
       "1     6.224047        504.742017  4.302428       73.878977\n",
       "2     3.132028         22.920412  2.013945        7.492819\n",
       "3     1.939077          6.952334  1.537005        4.650640\n",
       "4     1.532418          4.629359  1.272288        3.569008\n",
       "5     1.304426          3.685574  1.132638        3.103833\n",
       "6     1.143736          3.138472  1.027499        2.794068\n",
       "7     1.024340          2.785256  0.946582        2.576886\n",
       "8     0.927683          2.528643  0.891535        2.438871\n",
       "9     0.844745          2.327385  0.826734        2.285841\n",
       "10    0.767671          2.154741  0.779915        2.181287\n",
       "11    0.700491          2.014742  0.737892        2.091522\n",
       "12    0.634430          1.885947  0.698708        2.011153\n",
       "13    0.575762          1.778485  0.677022        1.968009\n",
       "14    0.528166          1.695819  0.649875        1.915301\n",
       "15    0.487052          1.627511  0.631373        1.880190\n",
       "16    0.456219          1.578096  0.618221        1.855625\n",
       "17    0.433538          1.542706  0.613006        1.845972\n",
       "18    0.422319          1.525495  0.607741        1.836279\n",
       "19    0.415934          1.515786  0.607744        1.836285"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.read_csv(\"training\\multi_modal\\metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0de0dc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionGPT2Model(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_1): GPT2CrossAttention(\n",
       "          (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn_2): GPT2CrossAttention(\n",
       "          (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = torch.load(\"training/multi_modal_withkeywords/captioner.pt\", weights_only=False)\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "87f47cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrv_keywords_extractor = XRayVisionKeywordsExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e13490fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart is normal in size and contour. The lungs are clear, without evidence of infiltrate. There is no pneumothorax or effusion.\n",
      "------------\n",
      " XXXX examination consists of frontal and lateral radiographs of the chest. The cardiomediastinal contours are within normal limits. Pulmonary vascularity is within normal limits. No focal consolidation, pleural effusion, or pneumothorax identified. The visualized osseous structures and upper abdomen are unremarkable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_caption(model, img_path, max_tokens=200, temperature=1.0, deterministic=False):\n",
    "    # model.eval()\n",
    "    gen_tfms = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = numpy.array(image)\n",
    "    image = gen_tfms(image=image)['image']\n",
    "    image = image.unsqueeze(0).to(device)  # Move the input image tensor to the same device as the model\n",
    "    sequence = torch.ones(1, 1).long().to(device) * tokenizer.bos_token_id\n",
    "\n",
    "    keywords = \"keywords: \" + \", \".join(xrv_keywords_extractor.extract([img_path])[0])\n",
    "    keywords = tokenizer(keywords, truncation=True, padding=\"max_length\", max_length=50, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    keywords = keywords.to(device)  # Move the keywords tensor to the same device as the model\n",
    "\n",
    "    caption = model.generate(\n",
    "        image,\n",
    "        sequence,\n",
    "        keywords,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        deterministic=deterministic\n",
    "    )\n",
    "    caption = tokenizer.decode(caption.cpu().numpy(), skip_special_tokens=True)  # Move the generated caption back to CPU for decoding\n",
    "\n",
    "    return caption\n",
    "\n",
    "img_path = os.path.join(images_folder, test_df.loc[100, \"filename\"])\n",
    "generated_report = generate_caption(best_model, img_path)\n",
    "print(test_df.loc[100, \"report\"])\n",
    "print(\"------------\")\n",
    "print(generated_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\v-mziadeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f28cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate BLEU score\n",
    "def compute_bleu(reference_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Compute BLEU score between generated texts and references.\n",
    "    \n",
    "    :param reference_texts: List of lists of reference texts (for each generated report)\n",
    "    :param generated_texts: List of generated reports\n",
    "    :return: BLEU score\n",
    "    \"\"\"\n",
    "    references = [[ref.split()] for ref in reference_texts]  # List of list of reference tokens\n",
    "    candidates = [gen.split() for gen in generated_texts]   # List of list of generated tokens\n",
    "    bleu_score = corpus_bleu(references, candidates)\n",
    "    return bleu_score\n",
    "\n",
    "# Function to calculate ROUGE score\n",
    "def compute_rouge(reference_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Compute ROUGE score between generated texts and references.\n",
    "    \n",
    "    :param reference_texts: List of reference reports\n",
    "    :param generated_texts: List of generated reports\n",
    "    :return: ROUGE score\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    \n",
    "    for reference, generated in zip(reference_texts, generated_texts):\n",
    "        scores = scorer.score(reference, generated)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(scores[key].fmeasure)\n",
    "    \n",
    "    avg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\n",
    "    avg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\n",
    "    avg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n",
    "    \n",
    "    return avg_rouge1, avg_rouge2, avg_rougeL\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, folder_path, eval_set):\n",
    "    generated_reports = []\n",
    "    reference_reports = []\n",
    "\n",
    "    print(f\"Starting evaluation for model using {len(eval_set)} items\")\n",
    "    for idx in range(len(eval_set)):\n",
    "        # Generate report for each image\n",
    "        generated_report = generate_caption(model, os.path.join(folder_path, eval_set.loc[idx, \"filename\"]))\n",
    "        reference_report = eval_set.loc[idx, \"report\"]\n",
    "        \n",
    "        generated_reports.append(generated_report)\n",
    "        reference_reports.append(reference_report)\n",
    "\n",
    "        print(f\"Generated report {idx + 1}/{len(eval_set)}    \\r\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # Compute BLEU\n",
    "    bleu_score = compute_bleu(reference_reports, generated_reports)\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    rouge1, rouge2, rougeL = compute_rouge(reference_reports, generated_reports)\n",
    "    print(f\"ROUGE-1: {rouge1:.4f}, ROUGE-2: {rouge2:.4f}, ROUGE-L: {rougeL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c41a6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for model using 747 items\n",
      "Generated report 747/747    \n",
      "BLEU Score: 0.0458\n",
      "ROUGE-1: 0.2948, ROUGE-2: 0.0945, ROUGE-L: 0.2019\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(best_model, images_folder, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_bleu_scores(reference_texts, generated_texts):\n",
    "    references = [[ref.split()] for ref in reference_texts]\n",
    "    candidates = [gen.split() for gen in generated_texts]\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "    bleu3 = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "    bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def compute_rouge_l(reference_texts, generated_texts):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, gen)[\"rougeL\"].fmeasure for ref, gen in zip(reference_texts, generated_texts)]\n",
    "    return numpy.mean(scores)\n",
    "\n",
    "def compute_cider(reference_texts, generated_texts):\n",
    "    # Simple CIDEr implementation for demonstration (not official)\n",
    "    # For real evaluation, use pycocoevalcap or similar library\n",
    "    def ngram_counts(text, n):\n",
    "        tokens = text.split()\n",
    "        return defaultdict(int, {tuple(tokens[i:i+n]): 1 for i in range(len(tokens)-n+1)})\n",
    "\n",
    "    cider_scores = []\n",
    "    for ref, gen in zip(reference_texts, generated_texts):\n",
    "        score = 0\n",
    "        for n in range(1, 5):\n",
    "            ref_ngrams = ngram_counts(ref, n)\n",
    "            gen_ngrams = ngram_counts(gen, n)\n",
    "            overlap = sum(min(gen_ngrams[ng], ref_ngrams[ng]) for ng in gen_ngrams)\n",
    "            total = max(len(gen.split())-n+1, 1)\n",
    "            score += overlap / total\n",
    "        cider_scores.append(score / 4)\n",
    "    return numpy.mean(cider_scores)\n",
    "\n",
    "# Generate reports for test set\n",
    "generated_reports = []\n",
    "reference_reports = []\n",
    "for idx in range(len(test_df)):\n",
    "    img_path = os.path.join(images_folder, test_df.loc[idx, \"filename\"])\n",
    "    generated_report = generate_caption(best_model, img_path)\n",
    "    reference_report = test_df.loc[idx, \"report\"]\n",
    "    generated_reports.append(generated_report)\n",
    "    reference_reports.append(reference_report)\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx+1}/{len(test_df)}\")\n",
    "\n",
    "bleu1, bleu2, bleu3, bleu4 = compute_bleu_scores(reference_reports, generated_reports)\n",
    "rouge_l = compute_rouge_l(reference_reports, generated_reports)\n",
    "cider = compute_cider(reference_reports, generated_reports)\n",
    "\n",
    "print(f\"BLEU-1: {bleu1:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "print(f\"BLEU-4: {bleu4:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_l:.4f}\")\n",
    "print(f\"CIDEr: {cider:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a92fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
